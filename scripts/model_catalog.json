[
  {
    "id": "llama3.1",
    "engine": "mlx",
    "sizeB": 8,
    "minRamGb": 8
  },
  {
    "id": "llama3.2",
    "engine": "mlx",
    "sizeB": 3,
    "minRamGb": 6
  },
  {
    "id": "gemma2:2b",
    "engine": "mlx",
    "sizeB": 2,
    "minRamGb": 4
  },
  {
    "id": "meta-llama/Llama-3.2-3B-Instruct",
    "engine": "vllm",
    "sizeB": 3,
    "minVramGb": 6
  },
  {
    "id": "meta-llama/Meta-Llama-3.1-8B-Instruct-AWQ",
    "engine": "vllm",
    "sizeB": 8,
    "minVramGb": 8,
    "quantization": "AWQ"
  },
  {
    "id": "meta-llama/Meta-Llama-3.1-8B-Instruct",
    "engine": "vllm",
    "sizeB": 8,
    "minVramGb": 10
  },
  {
    "id": "meta-llama/Meta-Llama-3.1-8B-Instruct",
    "engine": "tensorrt-llm",
    "sizeB": 8,
    "minVramGb": 10
  },
  {
    "id": "meta-llama/Meta-Llama-3.1-70B-Instruct",
    "engine": "tensorrt-llm",
    "sizeB": 70,
    "minVramGb": 80
  },
  {
    "id": "TheBloke/Llama-3.2-3B-Instruct-GGUF:Q4_K_M",
    "engine": "llama.cpp",
    "sizeB": 3,
    "minRamGb": 6,
    "quantization": "Q4_K_M"
  },
  {
    "id": "TheBloke/Llama-3.1-8B-Instruct-GGUF:Q4_K_M",
    "engine": "llama.cpp",
    "sizeB": 8,
    "minRamGb": 10,
    "quantization": "Q4_K_M"
  },
  {
    "id": "TheBloke/Llama-3.1-8B-Instruct-GGUF:Q5_K_M",
    "engine": "llama.cpp",
    "sizeB": 8,
    "minRamGb": 12,
    "quantization": "Q5_K_M"
  }
]
